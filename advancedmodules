I'll implement more sophisticated features for our quantum consciousness visualization simulator. Let's enhance each module with more advanced functionality.

```python
# models/complex_systems.py
import numpy as np
from scipy.integrate import solve_ivp

class ComplexSystemsModel:
    def __init__(self, parameters, dimensions=3, complexity_level=2):
        self.parameters = parameters
        self.dimensions = dimensions
        self.complexity_level = complexity_level
        self.state = np.random.rand(dimensions)
        
    def lorenz_attractor(self, t, state, sigma=10, rho=28, beta=8/3):
        """Lorenz system for chaotic behavior simulation"""
        x, y, z = state
        dx_dt = sigma * (y - x)
        dy_dt = x * (rho - z) - y
        dz_dt = x * y - beta * z
        return [dx_dt, dy_dt, dz_dt]
    
    def quantum_fluctuation(self, state):
        """Simulate quantum fluctuations in the system"""
        uncertainty = np.random.normal(0, 0.01, size=len(state))
        return state + uncertainty * self.complexity_level
    
    def simulate(self, duration=10.0, steps=1000):
        """Simulate complex system behavior with quantum effects"""
        t_span = (0, duration)
        t_eval = np.linspace(0, duration, steps)
        
        # Run deterministic simulation
        solution = solve_ivp(
            self.lorenz_attractor, 
            t_span, 
            self.state, 
            t_eval=t_eval
        )
        
        # Add quantum effects
        quantum_trajectory = np.zeros_like(solution.y)
        for i in range(solution.y.shape[1]):
            quantum_trajectory[:, i] = self.quantum_fluctuation(solution.y[:, i])
            
        return {
            'time': solution.t,
            'classical_states': solution.y,
            'quantum_states': quantum_trajectory
        }
```

```python
# models/interdisciplinary_insights.py
import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

class InterdisciplinaryModel:
    def __init__(self, domains=None):
        self.domains = domains or ['quantum_physics', 'neuroscience', 'complexity_theory']
        self.domain_weights = {domain: np.random.random() for domain in self.domains}
        self.insights = {}
        
    def add_domain_data(self, domain, data):
        """Add data from a specific knowledge domain"""
        if domain not in self.domains:
            self.domains.append(domain)
            self.domain_weights[domain] = np.random.random()
        self.insights[domain] = np.array(data)
        
    def normalize_weights(self):
        """Normalize domain weights to sum to 1"""
        total = sum(self.domain_weights.values())
        for domain in self.domain_weights:
            self.domain_weights[domain] /= total
            
    def extract_patterns(self, n_components=2):
        """Extract interdisciplinary patterns using PCA"""
        if not self.insights:
            return None
            
        # Combine insights from all domains
        combined_data = []
        for domain, data in self.insights.items():
            if len(data.shape) == 1:
                data = data.reshape(1, -1)
            weighted_data = data * self.domain_weights[domain]
            combined_data.append(weighted_data)
            
        if not combined_data:
            return None
            
        all_data = np.vstack(combined_data)
        
        # Apply PCA to find patterns
        pca = PCA(n_components=min(n_components, all_data.shape[0]))
        transformed = pca.fit_transform(all_data)
        
        # Cluster the patterns
        kmeans = KMeans(n_clusters=min(3, all_data.shape[0]))
        clusters = kmeans.fit_predict(transformed)
        
        return {
            'patterns': transformed,
            'explained_variance': pca.explained_variance_ratio_,
            'clusters': clusters,
            'cluster_centers': kmeans.cluster_centers_
        }
```

```python
# models/innovative_solutions.py
import numpy as np
from scipy.optimize import minimize

class InnovativeSolutions:
    def __init__(self, constraints=None, objectives=None):
        self.constraints = constraints or []
        self.objectives = objectives or []
        self.solutions = []
        
    def add_constraint(self, constraint_func):
        """Add a constraint function"""
        self.constraints.append(constraint_func)
        
    def add_objective(self, objective_func, weight=1.0):
        """Add an objective function with weight"""
        self.objectives.append((objective_func, weight))
        
    def combined_objective(self, x):
        """Combine multiple objectives with weights"""
        if not self.objectives:
            return 0
            
        total = 0
        for func, weight in self.objectives:
            total += weight * func(x)
        return total
    
    def generate_solutions(self, initial_guess, bounds=None):
        """Generate innovative solutions through optimization"""
        constraint_list = [{'type': 'ineq', 'fun': c} for c in self.constraints]
        
        result = minimize(
            self.combined_objective,
            initial_guess,
            method='SLSQP',
            bounds=bounds,
            constraints=constraint_list
        )
        
        self.solutions.append({
            'parameters': result.x,
            'objective_value': result.fun,
            'success': result.success,
            'message': result.message
        })
        
        return self.solutions[-1]
    
    def evolve_solutions(self, generations=5, mutation_rate=0.1):
        """Evolve existing solutions to find better ones"""
        if not self.solutions:
            return None
            
        evolved = []
        for _ in range(generations):
            # Select parents (best solutions)
            parents = sorted(self.solutions, key=lambda x: x['objective_value'])[:2]
            
            for parent in parents:
                # Create mutation
                mutation = parent['parameters'] + np.random.normal(0, mutation_rate, size=parent['parameters'].shape)
                
                # Generate new solution from mutation
                new_sol = self.generate_solutions(mutation)
                evolved.append(new_sol)
                
        return evolved
```

```python
# knowledge_integration/knowledge_mapping.py
import numpy as np
import networkx as nx

class KnowledgeMap:
    def __init__(self):
        self.graph = nx.DiGraph()
        self.knowledge_nodes = {}
        self.relationship_types = set()
        
    def add_knowledge_node(self, node_id, content, domain, confidence=1.0):
        """Add a knowledge node to the map"""
        self.graph.add_node(
            node_id, 
            content=content, 
            domain=domain, 
            confidence=confidence,
            timestamp=np.datetime64('now')
        )
        self.knowledge_nodes[node_id] = {
            'content': content,
            'domain': domain,
            'confidence': confidence
        }
        
    def add_relationship(self, source_id, target_id, relationship_type, strength=1.0):
        """Add a relationship between knowledge nodes"""
        if source_id in self.graph and target_id in self.graph:
            self.graph.add_edge(
                source_id, 
                target_id, 
                type=relationship_type, 
                strength=strength
            )
            self.relationship_types.add(relationship_type)
            
    def get_related_knowledge(self, node_id, relationship_type=None, max_depth=2):
        """Get knowledge related to a specific node"""
        if node_id not in self.graph:
            return []
            
        related = []
        
        if relationship_type:
            # Filter by relationship type
            for _, target, data in self.graph.out_edges(node_id, data=True):
                if data['type'] == relationship_type:
                    related.append((target, self.graph.nodes[target], data))
        else:
            # Get all relationships
            for source, target, data in nx.bfs_edges(self.graph, node_id, depth_limit=max_depth):
                related.append((target, self.graph.nodes[target], data))
                
        return related
    
    def find_knowledge_paths(self, source_id, target_id):
        """Find paths between knowledge nodes"""
        if source_id not in self.graph or target_id not in self.graph:
            return []
            
        try:
            paths = list(nx.all_simple_paths(self.graph, source_id, target_id))
            return paths
        except nx.NetworkXNoPath:
            return []
```

```python
# knowledge_integration/insight_integration.py
import numpy as np
from sklearn.manifold import TSNE
import networkx as nx

class InsightIntegration:
    def __init__(self, knowledge=None):
        self.knowledge = knowledge or {}
        self.integrated_insights = {}
        self.confidence_scores = {}
        
    def add_knowledge(self, domain, data, confidence=1.0):
        """Add knowledge from a specific domain"""
        self.knowledge[domain] = data
        self.confidence_scores[domain] = confidence
        
    def integrate(self, method='weighted_average'):
        """Integrate insights from different domains"""
        if not self.knowledge:
            return None
            
        if method == 'weighted_average':
            integrated = self._weighted_average_integration()
        elif method == 'network_based':
            integrated = self._network_based_integration()
        elif method == 'dimensional_reduction':
            integrated = self._dimensional_reduction_integration()
        else:
            integrated = self._weighted_average_integration()
            
        self.integrated_insights = integrated
        return integrated
    
    def _weighted_average_integration(self):
        """Integration using weighted averages based on confidence"""
        result = {}
        
        # Normalize confidence scores
        total_confidence = sum(self.confidence_scores.values())
        normalized_confidence = {
            domain: conf/total_confidence 
            for domain, conf in self.confidence_scores.items()
        }
        
        # Combine numeric data with weighted average
        numeric_domains = {}
        for domain, data in self.knowledge.items():
            if isinstance(data, (np.ndarray, list)) and all(isinstance(x, (int, float)) for x in np.array(data).flatten()):
                numeric_domains[domain] = np.array(data)
                
        if numeric_domains:
            weighted_sum = np.zeros_like(list(numeric_domains.values())[0], dtype=float)
            for domain, data in numeric_domains.items():
                weighted_sum += data * normalized_confidence[domain]
            result['numeric_integration'] = weighted_sum
            
        # Combine text/categorical data (simplified)
        result['domains_integrated'] = list(self.knowledge.keys())
        result['confidence_distribution'] = normalized_confidence
        
        return result
    
    def _network_based_integration(self):
        """Integration using network analysis"""
        G = nx.Graph()
        
        # Create nodes for each knowledge element
        for domain, data in self.knowledge.items():
            if isinstance(data, dict):
                for key, value in data.items():
                    node_id = f"{domain}_{key}"
                    G.add_node(node_id, domain=domain, key=key, value=value)
            elif isinstance(data, list):
                for i, item in enumerate(data):
                    node_id = f"{domain}_{i}"
                    G.add_node(node_id, domain=domain, index=i, value=item)
                    
        # Create edges between related nodes (simplified)
        domains = list(self.knowledge.keys())
        for i in range(len(domains)):
            for j in range(i+1, len(domains)):
                domain1, domain2 = domains[i], domains[j]
                for node1 in [n for n in G.nodes if G.nodes[n]['domain'] == domain1]:
                    for node2 in [n for n in G.nodes if G.nodes[n]['domain'] == domain2]:
                        # Simple similarity measure
                        G.add_edge(node1, node2, weight=0.5)
        
        # Analyze network
        result = {
            'centrality': nx.degree_centrality(G),
            'communities': list(nx.community.greedy_modularity_communities(G)),
            'graph': G
        }
        
        return result
    
    def _dimensional_reduction_integration(self):
        """Integration using dimensional reduction techniques"""
        # Convert knowledge to feature vectors (simplified)
        features = []
        labels = []
        
        for domain, data in self.knowledge.items():
            if isinstance(data, (np.ndarray, list)):
                arr = np.array(data)
                if arr.ndim == 1:
                    features.append(arr)
                    labels.append(domain)
                elif arr.ndim == 2:
                    for row in arr:
                        features.append(row)
                        labels.append(domain)
        
        if not features:
            return {'error': 'No suitable features for dimensional reduction'}
            
        # Ensure all features have same dimension by padding
        max_len = max(len(f) for f in features)
        padded_features = []
        for f in features:
            if len(f) < max_len:
                padded = np.pad(f, (0, max_len - len(f)), 'constant')
                padded_features.append(padded)
            else:
                padded_features.append(f)
                
        # Apply t-SNE for visualization
        if len(padded_features) >= 2:
            tsne = TSNE(n_components=2, random_state=42)
            embedded = tsne.fit_transform(padded_features)
            
            return {
                'embedded_points': embedded,
                'labels': labels,
                'original_dimensions': max_len,
                'reduced_dimensions': 2
            }
        else:
            return {'error': 'Not enough data points for dimensional reduction'}
```

```python
# knowledge_integration/model_refinement.py
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

class ModelRefinement:
    def __init__(self, base_model=None):
        self.base_model = base_model
        self.refinement_history = []
        self.performance_metrics = {}
        
    def set_base_model(self, model):
        """Set the base model to refine"""
        self.base_model = model
        
    def evaluate_model(self, X, y):
        """Evaluate model performance"""
        if not hasattr(self.base_model, 'predict'):
            return {'error': 'Model does not have predict method'}
            
        try:
            y_pred = self.base_model.predict(X)
            mse = mean_squared_error(y, y_pred)
            self.performance_metrics['mse'] = mse
            return {'mse': mse}
        except Exception as e:
            return {'error': str(e)}
    
    def refine_parameters(self, param_grid, X, y, cv=3):
        """Refine model parameters using grid search"""
        if not hasattr(self.base_model, 'fit'):
            return {'error': 'Model does not have fit method'}
            
        best_params = None
        best_score = float('inf')
        
        # Simple grid search implementation
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
        
        for params in self._generate_param_combinations(param_grid):
            try:
                # Set parameters
                for param, value in params.items():
                    setattr(self.base_model, param, value)
                    
                # Train and evaluate
                self.base_model.fit(X_train, y_train)
                y_pred = self.base_model.predict(X_val)
                score = mean_squared_error(y_val, y_pred)
                
                if score < best_score:
                    best_score = score
                    best_params = params.copy()
            except Exception as e:
                continue
                
        # Apply best parameters
        if best_params:
            for param, value in best_params.items():
                setattr(self.base_model, param, value)
                
        self.refinement_history.append({
            'type': 'parameter_tuning',
            'best_params': best_params,
            'best_score': best_score
        })
        
        return {
            'best_params': best_params,
            'best_score': best_score
        }
    
    def _generate_param_combinations(self, param_grid):
        """Generate all combinations of parameters"""
        keys = list(param_grid.keys())
        values = list(param_grid.values())
        
        for combination in self._recursive_grid(keys, values, 0, {}):
            yield combination
    
    def _recursive_grid(self, keys, values, index, current_combo):
        """Recursively generate parameter combinations"""
        if index >= len(keys):
            return [current_combo.copy()]
            
        result = []
        for val in values[index]:
            current_combo[keys[index]] = val
            result.extend(self._recursive_grid(keys, values, index + 1, current_combo))
            
        return result
```

```python
# timeline_synchronization/temporal_synchronization.py
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

class TemporalSynchronization:
    def __init__(self, timelines=None):
        self.timelines = timelines or {}
        self.reference_timeline = None
        self.sync_points = {}
        
    def add_timeline(self, timeline_id, events):
        """Add a timeline with its events"""
        if isinstance(events, list):
            # Convert to DataFrame if it's a list
            if events and isinstance(events[0], dict):
                self.timelines[timeline_id] = pd.DataFrame(events)
            else:
                self.timelines[timeline_id] = pd.DataFrame({
                    'time': range(len(events)),
                    'value': events
                })
        elif isinstance(events, pd.DataFrame):
            self.timelines[timeline_id] = events
        else:
            self.timelines[timeline_id] = pd.DataFrame({
                'time': [0],
                'value': [events]
            })
    
    def set_reference_timeline(self, timeline_id):
        """Set the reference timeline for synchronization"""
        if timeline_id in self.timelines:
            self.reference_timeline = timeline_id
        else:
            raise ValueError(f"Timeline {timeline_id} does not exist")
    
    def add_sync_point(self, timeline_id, reference_point, target_point):
        """Add a synchronization point between reference and target timeline"""
        if timeline_id not in self.timelines:
            raise ValueError(f"Timeline {timeline_id} does not exist")
            
        if timeline_id not in self.sync_points:
            self.sync_points[timeline_id] = []
            
        self.sync_points[timeline_id].append((reference_point, target_point))
    
    def synchronize(self, method='linear'):
        """Synchronize all timelines to the reference timeline"""
        if not self.reference_timeline:
            if self.timelines:
                self.reference_timeline = list(self.timelines.keys())[0]
            else:
                return {}
                
        result = {
            'reference': self.reference_timeline,
            'synchronized_timelines': {}
        }
        
        ref_timeline = self.timelines[self.reference_timeline]
        
        for timeline_id, timeline in self.timelines.items():
            if timeline_id == self.reference_timeline:
                result['synchronized_timelines'][timeline_id] = timeline
                continue
                
            # Get sync points or create default ones
            if timeline_id in self.sync_points and self.sync_points[timeline_id]:
                sync_points = self.sync_points[timeline_id]
            else:
                # Default: map start to start and end to end
                sync_points = [(0, 0), (len(ref_timeline)-1, len(timeline)-1)]
                
            # Perform synchronization based on method
            if method == 'linear':
                synced = self._linear_synchronization(timeline, sync_points, ref_timeline)
            elif method == 'dynamic_time_warping':
                synced = self._dtw_synchronization(timeline, ref_timeline)
            else:
                synced = self._linear_synchronization(timeline, sync_points, ref_timeline)
                
            result['synchronized_timelines'][timeline_id] = synced
            
        return result
    
    def _linear_synchronization(self, timeline, sync_points, ref_timeline):
        """Linear interpolation between sync points"""
        synced_timeline = pd.DataFrame(index=ref_timeline.index)
        
        # Sort sync points by reference point
        sync_points = sorted(sync_points, key=lambda x: x[0])
        
        # For each column in the timeline
        for col in timeline.columns:
            if col == 'time':
                synced_timeline['time'] = ref_timeline['time']
                continue
                
            synced_values = np.zeros(len(ref_timeline))
            
            # Process each segment between sync points
            for i in range(len(sync_points) - 1):
                ref_start, target_start = sync_points[i]
                ref_end, target_end = sync_points[i+1]
                
                # Calculate indices in reference timeline
                ref_start_idx = min(max(0, int(ref_start)), len(ref_timeline) - 1)
                ref_end_idx = min(max(0, int(ref_end)), len(ref_timeline) - 1)
                
                # Calculate indices in target timeline
                target_start_idx = min(max(0, int(target_start)), len(timeline) - 1)
                target_end_idx = min(max(0, int(target_end)), len(timeline) - 1)
                
                # Skip if indices are the same
                if ref_start_idx == ref_end_idx or target_start_idx == target_end_idx:
                    continue
                    
                # Get values from target timeline
                target_values = timeline[col].iloc[target_start_idx:target_end_idx+1].values
                
                # Interpolate to match reference timeline segment length
                segment_length = ref_end_idx - ref_start_idx + 1
                interpolated = np.interp(
                    np.linspace(0, 1, segment_length),
                    np.linspace(0, 1, len(target_values)),
                    target_values
                )
                
                # Assign interpolated values
                synced_values[ref_start_idx:ref_end_idx+1] = interpolated
                
            synced_timeline[col] = synced_values
            
        return synced_timeline
    
    def _dtw_synchronization(self, timeline, ref_timeline):
        """Dynamic Time Warping synchronization"""
        # Simplified DTW implementation
        synced_timeline = pd.DataFrame(index=ref_timeline.index)
        
        # Copy time column
        synced_timeline['time'] = ref_timeline['time']
        
        # For each column (except time)
        for col in timeline.columns:
            if col == 'time':
                continue
                
            # Get values
            target_values = timeline[col].values
            
            # Create a warping path (simplified)
            n, m = len(ref_timeline), len(target_values)
            path = []
            
            # Simple linear warping path
            for i in range(n):
                j = min(int(i * m / n), m-1)
                path.append((i, j))
                
            # Create synchronized values
            synced_values = np.zeros(n)
            for i, j in path:
                synced_values[i] = target_values[j]
                
            synced_timeline[col] = synced_values
            
        return synced_timeline
```

```python
# timeline_synchronization/knowledge_sharing.py
import numpy as np
import pandas as pd
from datetime import datetime

class KnowledgeSharing:
    def __init__(self):
        self.timelines = {}
        self.shared_knowledge = {}
        self.transfer_history = []
        
    def add_timeline(self, timeline_id, initial_knowledge=None):
        """Add a timeline with initial knowledge"""
        self.timelines[timeline_id] = initial_knowledge or {}
        
    def add_knowledge(self, timeline_id, knowledge_id, content, metadata=None):
        """Add knowledge to a specific timeline"""
        if timeline_id not in self.timelines:
            self.timelines[timeline_id] = {}
            
        self.timelines[timeline_id][knowledge_id] = {
            'content': content,
            'metadata': metadata or {},
            'timestamp': datetime.now().isoformat(),
            'shared_with': []
        }
        
    def share_knowledge(self, source_timeline, target_timeline, knowledge_ids=None, 
                        transformation=None, sharing_constraints=None):
        """Share knowledge between timelines with optional transformation"""
        if source_timeline not in self.timelines:
            return {'error': f'Source timeline {source_timeline} does not exist'}
            
        if target_timeline not in self.timelines:
            return {'error': f'Target timeline {target_timeline} does not exist'}
            
        # Determine which knowledge to share
        if knowledge_ids is None:
            # Share all knowledge
            to_share = list(self.timelines[source_timeline].keys())
        else:
            # Share specific knowledge
            to_share = [k for k in knowledge_ids if k in self.timelines[source_timeline]]
            
        if not to_share:
            return {'error': 'No knowledge to share'}
            
        # Apply sharing constraints
        if sharing_constraints:
            to_share = self._apply_sharing_constraints(to_share, sharing_constraints, 
                                                     source_timeline, target_timeline)
            
        # Share each knowledge item
        shared_items = []
        for knowledge_id in to_share:
            original = self.timelines[source_timeline][knowledge_id]
            
            # Apply transformation if provided
            if transformation:
                transformed_content = transformation(original['content'])
            else:
                transformed_content = original['content']
                
            # Create new knowledge ID for target timeline
            target_knowledge_id = f"{knowledge_id}_from_{source_timeline}"
            
            # Add to target timeline
            self.timelines[target_timeline][target_knowledge_id] = {
                'content': transformed_content,
                'metadata': {
                    **original['metadata'],
                    'source_timeline': source_timeline,
                    'original_id': knowledge_id,
                    'transfer_timestamp': datetime.now().isoformat()
                },
                'timestamp': datetime.now().isoformat(),
                'shared_with': []
            }
            
            # Update sharing history in source
            if target_timeline not in original['shared_with']:
                self.timelines[source_timeline][knowledge_id]['shared_with'].append(target_timeline)
                
            # Record the transfer
            transfer_record = {
                'source_timeline': source_timeline,
                'target_timeline': target_timeline,
                'knowledge_id': knowledge_id,
                'target_knowledge_id': target_knowledge_id,
                'timestamp': datetime.now().isoformat(),
                'transformed': transformation is not None
            }
            self.transfer_history.append(transfer_record)
            shared_items.append(transfer_record)
            
        return {
            'shared_items': shared_items,
            'count': len(shared_items)
        }
    
    def _apply_sharing_constraints(self, knowledge_ids, constraints, source, target):
        """Apply constraints to knowledge sharing"""
        allowed_ids = knowledge_ids.copy()
        
        for constraint in constraints:
            if constraint['type'] == 'exclude_metadata':
                # Exclude knowledge with specific metadata
                field = constraint['field']
                value = constraint['value']
                allowed_ids = [
                    k for k in allowed_ids 
                    if k in self.timelines[source] and
                    (field not in self.timelines[source][k]['metadata'] or
                     self.timelines[source][k]['metadata'][field] != value)
                ]
            elif constraint['type'] == 'require_metadata':
                # Require specific metadata
                field = constraint['field']
                value = constraint['value']
                allowed_ids = [
                    k for k in allowed_ids 
                    if k in self.timelines[source] and
                    field in self.timelines[source][k]['metadata'] and
                    self.timelines[source][k]['metadata'][field] == value
                ]
            elif constraint['type'] == 'max_items':
                # Limit number of items
                max_items = constraint['value']
                allowed_ids = allowed_ids[:max_items]
                
        return allowed_ids
    
    def get_shared_knowledge_graph(self):
        """Get a graph representation of knowledge sharing between timelines"""
        nodes = []
        edges = []
        
        # Create nodes for timelines
        for timeline_id in self.timelines:
            nodes.append({
                'id': timeline_id,
                'type': 'timeline',
                'knowledge_count': len(self.timelines[timeline_id])
            })
            
        # Create edges for knowledge transfers
        for transfer in self.transfer_history:
            edge_id = f"{transfer['source_timeline']}_{transfer['target_timeline']}_{transfer['knowledge_id']}"
            edges.append({
                'id': edge_id,
                'source': transfer['source_timeline'],
                'target': transfer['target_timeline'],
                'knowledge_id': transfer['knowledge_id'],
                'timestamp': transfer['timestamp']
            })
            
        return {
            'nodes': nodes,
            'edges': edges
        }
    
    def analyze_knowledge_flow(self):
        """Analyze the flow of knowledge between timelines"""
        if not self.transfer_history:
            return {'error': 'No knowledge transfers recorded'}
            
        # Convert to DataFrame for analysis
        df = pd.DataFrame(self.transfer_history)
        
        # Count transfers between timelines
        transfer_counts = df.groupby(['source_timeline', 'target_timeline']).size().reset_index()
        transfer_counts.columns = ['source', 'target', 'count']
        
        # Find most active timelines
        source_counts = df['source_timeline'].value_counts().to_dict()
        target_counts = df['target_timeline'].value_counts().to_dict()
        
        # Timeline influence (outgoing / total)
        all_timelines = set(source_counts.keys()).union(set(target_counts.keys()))
        influence = {}
        for timeline in all_timelines:
            outgoing = source_counts.get(timeline, 0)
            incoming = target_counts
